{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle or Colab environment and install packages accordingly\n",
    "import sys\n",
    "if 'kaggle' in sys.modules:\n",
    "    !pip install openai-whisper pyannote.audio gdown google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
    "elif 'google.colab' in sys.modules:\n",
    "    !pip install openai-whisper pyannote.audio gdown google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "from pyannote.core import Segment\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Import Kaggle Secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Function to authenticate and mount Google Drive\n",
    "def authenticate_and_mount_gdrive():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    authenticate_and_mount_gdrive()\n",
    "    audio_folder_path = \"/content/drive/MyDrive/Transcribe\"  # Update this path as per your folder structure\n",
    "    log_file_path = \"/content/drive/MyDrive/Transcribe/transcription_log.txt\"\n",
    "elif 'kaggle' in sys.modules:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"google_drive_api_key\")\n",
    "    authenticate_and_mount_gdrive()\n",
    "    audio_folder_path = \"/content/drive/MyDrive/Transcribe\"  # Update this path as per your folder structure\n",
    "    log_file_path = \"/content/drive/MyDrive/Transcribe/transcription_log.txt\"\n",
    "else:\n",
    "    # For local environments\n",
    "    audio_folder_path = \"./Transcribe\"  # Local directory\n",
    "    log_file_path = \"./transcription_log.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that Google Drive is mounted\n",
    "!ls /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "model_size = \"small\"  # Adjust based on your requirement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(message, log_file=log_file_path):\n",
    "    with open(log_file, \"a\") as file:\n",
    "        file.write(f\"{message}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_and_log_full_transcript(audio_path, model, log_file=log_file_path):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = model.transcribe(audio_path)\n",
    "        execution_time = time.time() - start_time\n",
    "        success_message = f\"Transcription successful for {Path(audio_path).name}! Execution time: {execution_time:.2f} seconds\"\n",
    "        log_message(success_message, log_file)\n",
    "        \n",
    "        output_file_path = Path(audio_path).with_suffix('.txt')\n",
    "        with open(output_file_path, \"w\") as file:\n",
    "            for segment in result['segments']:\n",
    "                file.write(f\"{segment['start']}-{segment['end']}: {segment['text']}\\n\")\n",
    "        \n",
    "        log_message(f\"Full transcription saved to {output_file_path}\", log_file)\n",
    "        return result['segments']\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during transcription of {Path(audio_path).name}: {str(e)}\"\n",
    "        log_message(error_message, log_file)\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_diarization_pyannote(audio_path, output_log=\"diarization_pyannote_log.txt\"):\n",
    "    start_time = time.time()\n",
    "    pipeline = SpeakerDiarization(segmentation=\"pyannote/segmentation\", use_auth_token=HfFolder.get_token())\n",
    "    diarization = pipeline({'uri': 'SpeakerDiarization', 'audio': audio_path})\n",
    "    \n",
    "    with open(output_log, \"w\") as log_file:\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            start, end = turn.start, turn.end\n",
    "            log_file.write(f\"Speaker: {speaker}, Start: {start}, End: {end}\\n\")\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    print(f\"Diarization completed successfully in {execution_time:.2f} seconds.\")\n",
    "    return [{'speaker': speaker, 'start': turn.start, 'end': turn.end} for turn, _, speaker in diarization.itertracks(yield_label=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_diarization_with_transcript(diarization_results, transcription_segments):\n",
    "    integrated_output = []\n",
    "    for segment in transcription_segments:\n",
    "        speaker_label = \"Unknown\"\n",
    "        for speaker_segment in diarization_results:\n",
    "            if segment['start'] >= speaker_segment['start'] and segment['end'] <= speaker_segment['end']:\n",
    "                speaker_label = speaker_segment['speaker']\n",
    "                break\n",
    "        integrated_output.append(f\"{speaker_label}: {segment['text']}\")\n",
    "    return integrated_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_directory(directory_path, model):\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if file_name.endswith(('.mp3', '.mp4', '.wav')):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            try:\n",
    "                transcription_segments = transcribe_and_log_full_transcript(file_path, model)\n",
    "                diarization_results = perform_diarization_pyannote(file_path)\n",
    "                integrated_transcript = integrate_diarization_with_transcript(diarization_results, transcription_segments)\n",
    "                \n",
    "                output_file_path = Path(file_path).with_suffix('.integrated.txt')\n",
    "                with open(output_file_path, \"w\") as file:\n",
    "                    for line in integrated_transcript:\n",
    "                        file.write(f\"{line}\\n\")\n",
    "                \n",
    "                print(f\"Integrated transcription saved to {output_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_files_in_directory(audio_folder_path, model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNpTNsCDYt6IEGL0VA1vZ5I",
   "gpuType": "T4",
   "mount_file_id": "1wk9qOfZK06RLBTEuy503MkQLIMNLmHXP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
