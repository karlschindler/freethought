{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 36920,
     "status": "ok",
     "timestamp": 1708356946584,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "Ar2S7HQraMwp",
    "outputId": "e6b25d71-517d-4662-ba49-bd0e6b74cb61"
   },
   "outputs": [],
   "source": [
    "!pip install openai-whisper\n",
    "!pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16936,
     "status": "ok",
     "timestamp": 1708356963493,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "aM9ssNrezk5g",
    "outputId": "debd7ae4-0cad-4464-a94a-35f1931a986f"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import pyannote.audio for Diarization\n",
    "import whisper\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "# Import pyannote.audio for diarization\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "from pyannote.core import Segment\n",
    "from huggingface_hub import HfFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1708356963494,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "Kw4emNz9dywo"
   },
   "outputs": [],
   "source": [
    "# Configuration Settings\n",
    "model_size = \"large\" # You can adjust this based on your requirement\n",
    "# Make sure to adjust the path after uploading the file or connecting Google Drive\n",
    "audio_file_path = \"/content/drive/MyDrive/Transcribe/social_network.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232114,
     "status": "ok",
     "timestamp": 1708357195604,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "JZxISROqzv9S",
    "outputId": "4feb9b2e-fad6-4839-d142-5251fba6e539"
   },
   "outputs": [],
   "source": [
    "# Load Whisper Model\n",
    "model = whisper.load_model(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26787,
     "status": "ok",
     "timestamp": 1708357222386,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "zebkZD4Czwo7"
   },
   "outputs": [],
   "source": [
    "# Transcription, Performance Metrics, Error Handling, Logging\n",
    "def transcribe_and_log_full_transcript(audio_path, model, log_file=\"transcription_log.txt\"):\n",
    "  start_time = time.time() # Start measuring time\n",
    "  try:\n",
    "    result = model.transcribe(audio_path)\n",
    "    execution_time = time.time() - start_time # Measure execution time\n",
    "\n",
    "    # Construct success message with execution time\n",
    "    success_message = f\"Transcription successful for {Path(audio_path).name}! Execution time: {execution_time:.2f} seconds\"\n",
    "    log_message(success_message, log_file) # Log the success message\n",
    "\n",
    "    # Save the full transcription to a text file in the same directory as the audio file\n",
    "    output_file_path = Path(audio_path).with_suffix('.txt')\n",
    "    with open(output_file_path, \"w\") as file:\n",
    "      for segment in result['segments']:\n",
    "        file.write(f\"{segment['start']}-{segment['end']}: {segment['text']}\\n\")\n",
    "\n",
    "    log_message(f\"Full transcription saved to {output_file_path}\", log_file)\n",
    "\n",
    "  except Exception as e:\n",
    "    # Log error message\n",
    "    error_message = f\"Error during transcription of {Path(audio_path).name}: {e}\"\n",
    "    log_message(error_message, log_file)\n",
    "\n",
    "def log_message(message, log_file=\"transcription_log.txt\"):\n",
    "  with open(log_file, \"a\") as file:\n",
    "    file.write(f\"{message}\\n\")\n",
    "\n",
    "transcribe_and_log_full_transcript(audio_file_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "executionInfo": {
     "elapsed": 1722,
     "status": "error",
     "timestamp": 1708357224099,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "1WJC7ZU5z37r",
    "outputId": "7a2f41f9-589c-4d85-9b93-afe4f2b8a687"
   },
   "outputs": [],
   "source": [
    "# Perform Speaker Diarization with pyannote.audio\n",
    "def perform_diarization_pyannote(audio_path, output_log=\"diarization_pyannote_log.txt\"):\n",
    "  \"\"\"Performs speaker diarization using pyannote.audio and logs the results.\"\"\"\n",
    "  # Initialize the speaker diarization pipeline with the default model\n",
    "  pipeline = SpeakerDiarization(segmentation=\"pyannote/segmentation\", use_auth_token=HfFolder.get_token())\n",
    "\n",
    "  # Apply the pipeline on the audio file\n",
    "  diarization = pipeline({'uri': 'SpeakerDiarization', 'audio': audio_path})\n",
    "\n",
    "  # Log or process diarization results\n",
    "  with open(output_log, \"w\") as log_file:\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "      start, end = turn.start, turn.end\n",
    "      log_file.write(f\"Speaker: {speaker}, Start: {start}, End: {end}\\n\")\n",
    "\n",
    "  print(\"Diarization completed successfully.\")\n",
    "\n",
    "# Execute diarization\n",
    "diarization_results = perform_diarization_pyannote(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1708357224100,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "_h4KkNsHbpup"
   },
   "outputs": [],
   "source": [
    "# Integrate Diarization Results with Transcription\n",
    "def integrate_diarization_with_transcript(diarization_results, transcription_segments):\n",
    "    \"\"\"\n",
    "    Integrates diarization results with transcription segments.\n",
    "    diarization_results: List of tuples or dicts with speaker ID and start/end times.\n",
    "    transcription_segments: List of dicts from Whisper transcription with 'text', 'start', and 'end'.\n",
    "    \"\"\"\n",
    "    # Assuming diarization_results is a list of dicts with 'speaker', 'start', and 'end'\n",
    "    integrated_output = []\n",
    "\n",
    "    for segment in transcription_segments:\n",
    "        # Find matching diarization segment based on time overlap\n",
    "        speaker_label = \"Unknown\"\n",
    "        for speaker_segment in diarization_results:\n",
    "            if segment['start'] >= speaker_segment['start'] and segment['end'] <= speaker_segment['end']:\n",
    "                speaker_label = speaker_segment['speaker']\n",
    "                break\n",
    "\n",
    "        integrated_output.append(f\"{speaker_label}: {segment['text']}\")\n",
    "\n",
    "    return integrated_output\n",
    "\n",
    "# Example usage:\n",
    "# Load or define your diarization_results and transcription_segments based on actual outputs from pyannote and Whisper\n",
    "# In this example, we're using placeholder lists for demonstration purposes.\n",
    "diarization_results = [{'speaker': 'Speaker 1', 'start': 0, 'end': 10}, {'speaker': 'Speaker 2', 'start': 10, 'end': 20}]\n",
    "transcription_segments = [{'text': \"Hello, this is an example.\", 'start': 0, 'end': 5}, {'text': \"Another speaker here.\", 'start': 10, 'end': 15}]\n",
    "\n",
    "# Integrate and print the combined output\n",
    "integrated_transcript = integrate_diarization_with_transcript(diarization_results, transcription_segments)\n",
    "for line in integrated_transcript:\n",
    "  print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "aborted",
     "timestamp": 1708357224100,
     "user": {
      "displayName": "Karl",
      "userId": "17827383527670862877"
     },
     "user_tz": -60
    },
    "id": "G0r4OxvXz6XU"
   },
   "outputs": [],
   "source": [
    "# Placeholder for future automation logic\n",
    "# Here you can add code to check Google Drive for new files, trigger transcription, etc.\n",
    "# This will be implemented when you're ready to automate the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwTgqO07ZuQE"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNpTNsCDYt6IEGL0VA1vZ5I",
   "gpuType": "T4",
   "mount_file_id": "1wk9qOfZK06RLBTEuy503MkQLIMNLmHXP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
